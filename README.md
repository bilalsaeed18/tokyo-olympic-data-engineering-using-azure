# tokyo-olympic-data-engineering-using-azure
tokyo-olympic-azure-data-engineering-project

Project Description: Tokyo Olympic Data Engineering Using Azure

In this comprehensive end-to-end project, I built a complete data engineering pipeline using various Azure services to process and visualize data related to the Tokyo Olympics. The goal was to handle large-scale data effectively and transform it into actionable insights.

- Azure Data Factory was used for orchestrating data pipelines, including the extraction of raw data from multiple sources such as CSV files and APIs. This data was then transformed using custom mappings and enriched before loading it into the data lake for further analysis.

- Azure Data Lake Storage Gen2 was utilized to store large volumes of structured and unstructured data efficiently, offering scalable and cost-effective data storage.

- Azure Synapse Analyticsserved as the data warehouse, where data was cleansed, aggregated, and transformed using SQL queries, Spark pools, and serverless SQL pools for faster processing and storage optimization. 

- Azure Databricks played a pivotal role in advanced data processing, running machine learning models, and batch processing. Here, I implemented distributed computing to ensure that massive datasets were processed in real-time with minimal latency. 

- Power BI was employed to create highly interactive dashboards to visualize the data insights. These dashboards illustrated various metrics such as the number of medals won by different countries, athlete performance trends, historical data comparisons, and event-wise statistics.

By combining the power of these Azure services, the project provided a scalable and efficient solution to handle complex datasets, offering advanced insights through interactive visualizations and demonstrating the flexibility of Azure for data engineering and analytics workflows.